"use strict";(globalThis.webpackChunkdocs_workspace=globalThis.webpackChunkdocs_workspace||[]).push([[3078],{87880(e,n,s){s.r(n),s.d(n,{assets:()=>t,contentTitle:()=>c,default:()=>u,frontMatter:()=>i,metadata:()=>l,toc:()=>a});const l=JSON.parse('{"id":"concepts/queue","title":"Command Queue (2026-01-16)","description":"We serialize inbound auto-reply runs (all channels) through a tiny in-process queue to prevent multiple agent runs from colliding, while still allowing safe parallelism across sessions.","source":"@site/content/moltbot/docs/concepts/queue.md","sourceDirName":"concepts","slug":"/concepts/queue","permalink":"/moltbot-doc/docs/concepts/queue","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"summary":"Command queue design that serializes inbound auto-reply runs","read_when":["Changing auto-reply execution or concurrency"]}}');var o=s(74848),r=s(28453);const i={summary:"Command queue design that serializes inbound auto-reply runs",read_when:["Changing auto-reply execution or concurrency"]},c="Command Queue (2026-01-16)",t={},a=[{value:"Why",id:"why",level:2},{value:"How it works",id:"how-it-works",level:2},{value:"Queue modes (per channel)",id:"queue-modes-per-channel",level:2},{value:"Queue options",id:"queue-options",level:2},{value:"Per-session overrides",id:"per-session-overrides",level:2},{value:"Scope and guarantees",id:"scope-and-guarantees",level:2},{value:"Troubleshooting",id:"troubleshooting",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"command-queue-2026-01-16",children:"Command Queue (2026-01-16)"})}),"\n",(0,o.jsx)(n.p,{children:"We serialize inbound auto-reply runs (all channels) through a tiny in-process queue to prevent multiple agent runs from colliding, while still allowing safe parallelism across sessions."}),"\n",(0,o.jsx)(n.h2,{id:"why",children:"Why"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Auto-reply runs can be expensive (LLM calls) and can collide when multiple inbound messages arrive close together."}),"\n",(0,o.jsx)(n.li,{children:"Serializing avoids competing for shared resources (session files, logs, CLI stdin) and reduces the chance of upstream rate limits."}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"how-it-works",children:"How it works"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"A lane-aware FIFO queue drains each lane with a configurable concurrency cap (default 1 for unconfigured lanes; main defaults to 4, subagent to 8)."}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"runEmbeddedPiAgent"})," enqueues by ",(0,o.jsx)(n.strong,{children:"session key"})," (lane ",(0,o.jsx)(n.code,{children:"session:<key>"}),") to guarantee only one active run per session."]}),"\n",(0,o.jsxs)(n.li,{children:["Each session run is then queued into a ",(0,o.jsx)(n.strong,{children:"global lane"})," (",(0,o.jsx)(n.code,{children:"main"})," by default) so overall parallelism is capped by ",(0,o.jsx)(n.code,{children:"agents.defaults.maxConcurrent"}),"."]}),"\n",(0,o.jsx)(n.li,{children:"When verbose logging is enabled, queued runs emit a short notice if they waited more than ~2s before starting."}),"\n",(0,o.jsx)(n.li,{children:"Typing indicators still fire immediately on enqueue (when supported by the channel) so user experience is unchanged while we wait our turn."}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"queue-modes-per-channel",children:"Queue modes (per channel)"}),"\n",(0,o.jsx)(n.p,{children:"Inbound messages can steer the current run, wait for a followup turn, or do both:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"steer"}),": inject immediately into the current run (cancels pending tool calls after the next tool boundary). If not streaming, falls back to followup."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"followup"}),": enqueue for the next agent turn after the current run ends."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"collect"}),": coalesce all queued messages into a ",(0,o.jsx)(n.strong,{children:"single"})," followup turn (default). If messages target different channels/threads, they drain individually to preserve routing."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"steer-backlog"})," (aka ",(0,o.jsx)(n.code,{children:"steer+backlog"}),"): steer now ",(0,o.jsx)(n.strong,{children:"and"})," preserve the message for a followup turn."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"interrupt"})," (legacy): abort the active run for that session, then run the newest message."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"queue"})," (legacy alias): same as ",(0,o.jsx)(n.code,{children:"steer"}),"."]}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:["Steer-backlog means you can get a followup response after the steered run, so\nstreaming surfaces can look like duplicates. Prefer ",(0,o.jsx)(n.code,{children:"collect"}),"/",(0,o.jsx)(n.code,{children:"steer"})," if you want\none response per inbound message.\nSend ",(0,o.jsx)(n.code,{children:"/queue collect"})," as a standalone command (per-session) or set ",(0,o.jsx)(n.code,{children:'messages.queue.byChannel.discord: "collect"'}),"."]}),"\n",(0,o.jsx)(n.p,{children:"Defaults (when unset in config):"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["All surfaces \u2192 ",(0,o.jsx)(n.code,{children:"collect"})]}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:["Configure globally or per channel via ",(0,o.jsx)(n.code,{children:"messages.queue"}),":"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-json5",children:'{\n  messages: {\n    queue: {\n      mode: "collect",\n      debounceMs: 1000,\n      cap: 20,\n      drop: "summarize",\n      byChannel: { discord: "collect" }\n    }\n  }\n}\n'})}),"\n",(0,o.jsx)(n.h2,{id:"queue-options",children:"Queue options"}),"\n",(0,o.jsxs)(n.p,{children:["Options apply to ",(0,o.jsx)(n.code,{children:"followup"}),", ",(0,o.jsx)(n.code,{children:"collect"}),", and ",(0,o.jsx)(n.code,{children:"steer-backlog"})," (and to ",(0,o.jsx)(n.code,{children:"steer"})," when it falls back to followup):"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"debounceMs"}),": wait for quiet before starting a followup turn (prevents \u201ccontinue, continue\u201d)."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"cap"}),": max queued messages per session."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"drop"}),": overflow policy (",(0,o.jsx)(n.code,{children:"old"}),", ",(0,o.jsx)(n.code,{children:"new"}),", ",(0,o.jsx)(n.code,{children:"summarize"}),")."]}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:["Summarize keeps a short bullet list of dropped messages and injects it as a synthetic followup prompt.\nDefaults: ",(0,o.jsx)(n.code,{children:"debounceMs: 1000"}),", ",(0,o.jsx)(n.code,{children:"cap: 20"}),", ",(0,o.jsx)(n.code,{children:"drop: summarize"}),"."]}),"\n",(0,o.jsx)(n.h2,{id:"per-session-overrides",children:"Per-session overrides"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["Send ",(0,o.jsx)(n.code,{children:"/queue <mode>"})," as a standalone command to store the mode for the current session."]}),"\n",(0,o.jsxs)(n.li,{children:["Options can be combined: ",(0,o.jsx)(n.code,{children:"/queue collect debounce:2s cap:25 drop:summarize"})]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"/queue default"})," or ",(0,o.jsx)(n.code,{children:"/queue reset"})," clears the session override."]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"scope-and-guarantees",children:"Scope and guarantees"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Applies to auto-reply agent runs across all inbound channels that use the gateway reply pipeline (WhatsApp web, Telegram, Slack, Discord, Signal, iMessage, webchat, etc.)."}),"\n",(0,o.jsxs)(n.li,{children:["Default lane (",(0,o.jsx)(n.code,{children:"main"}),") is process-wide for inbound + main heartbeats; set ",(0,o.jsx)(n.code,{children:"agents.defaults.maxConcurrent"})," to allow multiple sessions in parallel."]}),"\n",(0,o.jsxs)(n.li,{children:["Additional lanes may exist (e.g. ",(0,o.jsx)(n.code,{children:"cron"}),", ",(0,o.jsx)(n.code,{children:"subagent"}),") so background jobs can run in parallel without blocking inbound replies."]}),"\n",(0,o.jsx)(n.li,{children:"Per-session lanes guarantee that only one agent run touches a given session at a time."}),"\n",(0,o.jsx)(n.li,{children:"No external dependencies or background worker threads; pure TypeScript + promises."}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"If commands seem stuck, enable verbose logs and look for \u201cqueued for \u2026ms\u201d lines to confirm the queue is draining."}),"\n",(0,o.jsx)(n.li,{children:"If you need queue depth, enable verbose logs and watch for queue timing lines."}),"\n"]})]})}function u(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},28453(e,n,s){s.d(n,{R:()=>i,x:()=>c});var l=s(96540);const o={},r=l.createContext(o);function i(e){const n=l.useContext(r);return l.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:i(e.components),l.createElement(r.Provider,{value:n},e.children)}}}]);