"use strict";(globalThis.webpackChunkdocs_workspace=globalThis.webpackChunkdocs_workspace||[]).push([[2020],{39685(e,n,i){i.r(n),i.d(n,{assets:()=>a,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>s,toc:()=>t});const s=JSON.parse('{"id":"nodes/media-understanding","title":"Media Understanding (Inbound) \u2014 2026-01-17","description":"Moltbot can summarize inbound media (image/audio/video) before the reply pipeline runs. It auto\u2011detects when local tools or provider keys are available, and can be disabled or customized. If understanding is off, models still receive the original files/URLs as usual.","source":"@site/content/moltbot/docs/nodes/media-understanding.md","sourceDirName":"nodes","slug":"/nodes/media-understanding","permalink":"/moltbot-doc/docs/nodes/media-understanding","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"summary":"Inbound image/audio/video understanding (optional) with provider + CLI fallbacks","read_when":["Designing or refactoring media understanding","Tuning inbound audio/video/image preprocessing"]}}');var d=i(74848),l=i(28453);const r={summary:"Inbound image/audio/video understanding (optional) with provider + CLI fallbacks",read_when:["Designing or refactoring media understanding","Tuning inbound audio/video/image preprocessing"]},o="Media Understanding (Inbound) \u2014 2026-01-17",a={},t=[{value:"Goals",id:"goals",level:2},{value:"High\u2011level behavior",id:"highlevel-behavior",level:2},{value:"Config overview",id:"config-overview",level:2},{value:"Model entries",id:"model-entries",level:3},{value:"Defaults and limits",id:"defaults-and-limits",level:2},{value:"Auto-detect media understanding (default)",id:"auto-detect-media-understanding-default",level:3},{value:"Capabilities (optional)",id:"capabilities-optional",level:2},{value:"Provider support matrix (Moltbot integrations)",id:"provider-support-matrix-moltbot-integrations",level:2},{value:"Recommended providers",id:"recommended-providers",level:2},{value:"Attachment policy",id:"attachment-policy",level:2},{value:"Config examples",id:"config-examples",level:2},{value:"1) Shared models list + overrides",id:"1-shared-models-list--overrides",level:3},{value:"2) Audio + Video only (image off)",id:"2-audio--video-only-image-off",level:3},{value:"3) Optional image understanding",id:"3-optional-image-understanding",level:3},{value:"4) Multi\u2011modal single entry (explicit capabilities)",id:"4-multimodal-single-entry-explicit-capabilities",level:3},{value:"Status output",id:"status-output",level:2},{value:"Notes",id:"notes",level:2},{value:"Related docs",id:"related-docs",level:2}];function c(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,l.R)(),...e.components};return(0,d.jsxs)(d.Fragment,{children:[(0,d.jsx)(n.header,{children:(0,d.jsx)(n.h1,{id:"media-understanding-inbound--2026-01-17",children:"Media Understanding (Inbound) \u2014 2026-01-17"})}),"\n",(0,d.jsxs)(n.p,{children:["Moltbot can ",(0,d.jsx)(n.strong,{children:"summarize inbound media"})," (image/audio/video) before the reply pipeline runs. It auto\u2011detects when local tools or provider keys are available, and can be disabled or customized. If understanding is off, models still receive the original files/URLs as usual."]}),"\n",(0,d.jsx)(n.h2,{id:"goals",children:"Goals"}),"\n",(0,d.jsxs)(n.ul,{children:["\n",(0,d.jsx)(n.li,{children:"Optional: pre\u2011digest inbound media into short text for faster routing + better command parsing."}),"\n",(0,d.jsx)(n.li,{children:"Preserve original media delivery to the model (always)."}),"\n",(0,d.jsxs)(n.li,{children:["Support ",(0,d.jsx)(n.strong,{children:"provider APIs"})," and ",(0,d.jsx)(n.strong,{children:"CLI fallbacks"}),"."]}),"\n",(0,d.jsx)(n.li,{children:"Allow multiple models with ordered fallback (error/size/timeout)."}),"\n"]}),"\n",(0,d.jsx)(n.h2,{id:"highlevel-behavior",children:"High\u2011level behavior"}),"\n",(0,d.jsxs)(n.ol,{children:["\n",(0,d.jsxs)(n.li,{children:["Collect inbound attachments (",(0,d.jsx)(n.code,{children:"MediaPaths"}),", ",(0,d.jsx)(n.code,{children:"MediaUrls"}),", ",(0,d.jsx)(n.code,{children:"MediaTypes"}),")."]}),"\n",(0,d.jsxs)(n.li,{children:["For each enabled capability (image/audio/video), select attachments per policy (default: ",(0,d.jsx)(n.strong,{children:"first"}),")."]}),"\n",(0,d.jsx)(n.li,{children:"Choose the first eligible model entry (size + capability + auth)."}),"\n",(0,d.jsxs)(n.li,{children:["If a model fails or the media is too large, ",(0,d.jsx)(n.strong,{children:"fall back to the next entry"}),"."]}),"\n",(0,d.jsxs)(n.li,{children:["On success:\n",(0,d.jsxs)(n.ul,{children:["\n",(0,d.jsxs)(n.li,{children:[(0,d.jsx)(n.code,{children:"Body"})," becomes ",(0,d.jsx)(n.code,{children:"[Image]"}),", ",(0,d.jsx)(n.code,{children:"[Audio]"}),", or ",(0,d.jsx)(n.code,{children:"[Video]"})," block."]}),"\n",(0,d.jsxs)(n.li,{children:["Audio sets ",(0,d.jsx)(n.code,{children:"{{Transcript}}"}),"; command parsing uses caption text when present,\notherwise the transcript."]}),"\n",(0,d.jsxs)(n.li,{children:["Captions are preserved as ",(0,d.jsx)(n.code,{children:"User text:"})," inside the block."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,d.jsxs)(n.p,{children:["If understanding fails or is disabled, ",(0,d.jsx)(n.strong,{children:"the reply flow continues"})," with the original body + attachments."]}),"\n",(0,d.jsx)(n.h2,{id:"config-overview",children:"Config overview"}),"\n",(0,d.jsxs)(n.p,{children:[(0,d.jsx)(n.code,{children:"tools.media"})," supports ",(0,d.jsx)(n.strong,{children:"shared models"})," plus per\u2011capability overrides:"]}),"\n",(0,d.jsxs)(n.ul,{children:["\n",(0,d.jsxs)(n.li,{children:[(0,d.jsx)(n.code,{children:"tools.media.models"}),": shared model list (use ",(0,d.jsx)(n.code,{children:"capabilities"})," to gate)."]}),"\n",(0,d.jsxs)(n.li,{children:[(0,d.jsx)(n.code,{children:"tools.media.image"})," / ",(0,d.jsx)(n.code,{children:"tools.media.audio"})," / ",(0,d.jsx)(n.code,{children:"tools.media.video"}),":\n",(0,d.jsxs)(n.ul,{children:["\n",(0,d.jsxs)(n.li,{children:["defaults (",(0,d.jsx)(n.code,{children:"prompt"}),", ",(0,d.jsx)(n.code,{children:"maxChars"}),", ",(0,d.jsx)(n.code,{children:"maxBytes"}),", ",(0,d.jsx)(n.code,{children:"timeoutSeconds"}),", ",(0,d.jsx)(n.code,{children:"language"}),")"]}),"\n",(0,d.jsxs)(n.li,{children:["provider overrides (",(0,d.jsx)(n.code,{children:"baseUrl"}),", ",(0,d.jsx)(n.code,{children:"headers"}),", ",(0,d.jsx)(n.code,{children:"providerOptions"}),")"]}),"\n",(0,d.jsxs)(n.li,{children:["Deepgram audio options via ",(0,d.jsx)(n.code,{children:"tools.media.audio.providerOptions.deepgram"})]}),"\n",(0,d.jsxs)(n.li,{children:["optional ",(0,d.jsxs)(n.strong,{children:["per\u2011capability ",(0,d.jsx)(n.code,{children:"models"})," list"]})," (preferred before shared models)"]}),"\n",(0,d.jsxs)(n.li,{children:[(0,d.jsx)(n.code,{children:"attachments"})," policy (",(0,d.jsx)(n.code,{children:"mode"}),", ",(0,d.jsx)(n.code,{children:"maxAttachments"}),", ",(0,d.jsx)(n.code,{children:"prefer"}),")"]}),"\n",(0,d.jsxs)(n.li,{children:[(0,d.jsx)(n.code,{children:"scope"})," (optional gating by channel/chatType/session key)"]}),"\n"]}),"\n"]}),"\n",(0,d.jsxs)(n.li,{children:[(0,d.jsx)(n.code,{children:"tools.media.concurrency"}),": max concurrent capability runs (default ",(0,d.jsx)(n.strong,{children:"2"}),")."]}),"\n"]}),"\n",(0,d.jsx)(n.pre,{children:(0,d.jsx)(n.code,{className:"language-json5",children:"{\n  tools: {\n    media: {\n      models: [ /* shared list */ ],\n      image: { /* optional overrides */ },\n      audio: { /* optional overrides */ },\n      video: { /* optional overrides */ }\n    }\n  }\n}\n"})}),"\n",(0,d.jsx)(n.h3,{id:"model-entries",children:"Model entries"}),"\n",(0,d.jsxs)(n.p,{children:["Each ",(0,d.jsx)(n.code,{children:"models[]"})," entry can be ",(0,d.jsx)(n.strong,{children:"provider"})," or ",(0,d.jsx)(n.strong,{children:"CLI"}),":"]}),"\n",(0,d.jsx)(n.pre,{children:(0,d.jsx)(n.code,{className:"language-json5",children:'{\n  type: "provider",        // default if omitted\n  provider: "openai",\n  model: "gpt-5.2",\n  prompt: "Describe the image in <= 500 chars.",\n  maxChars: 500,\n  maxBytes: 10485760,\n  timeoutSeconds: 60,\n  capabilities: ["image"], // optional, used for multi\u2011modal entries\n  profile: "vision-profile",\n  preferredProfile: "vision-fallback"\n}\n'})}),"\n",(0,d.jsx)(n.pre,{children:(0,d.jsx)(n.code,{className:"language-json5",children:'{\n  type: "cli",\n  command: "gemini",\n  args: [\n    "-m",\n    "gemini-3-flash",\n    "--allowed-tools",\n    "read_file",\n    "Read the media at {{MediaPath}} and describe it in <= {{MaxChars}} characters."\n  ],\n  maxChars: 500,\n  maxBytes: 52428800,\n  timeoutSeconds: 120,\n  capabilities: ["video", "image"]\n}\n'})}),"\n",(0,d.jsx)(n.p,{children:"CLI templates can also use:"}),"\n",(0,d.jsxs)(n.ul,{children:["\n",(0,d.jsxs)(n.li,{children:[(0,d.jsx)(n.code,{children:"{{MediaDir}}"})," (directory containing the media file)"]}),"\n",(0,d.jsxs)(n.li,{children:[(0,d.jsx)(n.code,{children:"{{OutputDir}}"})," (scratch dir created for this run)"]}),"\n",(0,d.jsxs)(n.li,{children:[(0,d.jsx)(n.code,{children:"{{OutputBase}}"})," (scratch file base path, no extension)"]}),"\n"]}),"\n",(0,d.jsx)(n.h2,{id:"defaults-and-limits",children:"Defaults and limits"}),"\n",(0,d.jsx)(n.p,{children:"Recommended defaults:"}),"\n",(0,d.jsxs)(n.ul,{children:["\n",(0,d.jsxs)(n.li,{children:[(0,d.jsx)(n.code,{children:"maxChars"}),": ",(0,d.jsx)(n.strong,{children:"500"})," for image/video (short, command\u2011friendly)"]}),"\n",(0,d.jsxs)(n.li,{children:[(0,d.jsx)(n.code,{children:"maxChars"}),": ",(0,d.jsx)(n.strong,{children:"unset"})," for audio (full transcript unless you set a limit)"]}),"\n",(0,d.jsxs)(n.li,{children:[(0,d.jsx)(n.code,{children:"maxBytes"}),":\n",(0,d.jsxs)(n.ul,{children:["\n",(0,d.jsxs)(n.li,{children:["image: ",(0,d.jsx)(n.strong,{children:"10MB"})]}),"\n",(0,d.jsxs)(n.li,{children:["audio: ",(0,d.jsx)(n.strong,{children:"20MB"})]}),"\n",(0,d.jsxs)(n.li,{children:["video: ",(0,d.jsx)(n.strong,{children:"50MB"})]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,d.jsx)(n.p,{children:"Rules:"}),"\n",(0,d.jsxs)(n.ul,{children:["\n",(0,d.jsxs)(n.li,{children:["If media exceeds ",(0,d.jsx)(n.code,{children:"maxBytes"}),", that model is skipped and the ",(0,d.jsx)(n.strong,{children:"next model is tried"}),"."]}),"\n",(0,d.jsxs)(n.li,{children:["If the model returns more than ",(0,d.jsx)(n.code,{children:"maxChars"}),", output is trimmed."]}),"\n",(0,d.jsxs)(n.li,{children:[(0,d.jsx)(n.code,{children:"prompt"})," defaults to simple \u201cDescribe the ",(0,d.jsx)(n.code,{children:"{media}"}),".\u201d plus the ",(0,d.jsx)(n.code,{children:"maxChars"})," guidance (image/video only)."]}),"\n",(0,d.jsxs)(n.li,{children:["If ",(0,d.jsx)(n.code,{children:"<capability>.enabled: true"})," but no models are configured, Moltbot tries the\n",(0,d.jsx)(n.strong,{children:"active reply model"})," when its provider supports the capability."]}),"\n"]}),"\n",(0,d.jsx)(n.h3,{id:"auto-detect-media-understanding-default",children:"Auto-detect media understanding (default)"}),"\n",(0,d.jsxs)(n.p,{children:["If ",(0,d.jsx)(n.code,{children:"tools.media.<capability>.enabled"})," is ",(0,d.jsx)(n.strong,{children:"not"})," set to ",(0,d.jsx)(n.code,{children:"false"})," and you haven\u2019t\nconfigured models, Moltbot auto-detects in this order and ",(0,d.jsx)(n.strong,{children:"stops at the first\nworking option"}),":"]}),"\n",(0,d.jsxs)(n.ol,{children:["\n",(0,d.jsxs)(n.li,{children:[(0,d.jsx)(n.strong,{children:"Local CLIs"})," (audio only; if installed)\n",(0,d.jsxs)(n.ul,{children:["\n",(0,d.jsxs)(n.li,{children:[(0,d.jsx)(n.code,{children:"sherpa-onnx-offline"})," (requires ",(0,d.jsx)(n.code,{children:"SHERPA_ONNX_MODEL_DIR"})," with encoder/decoder/joiner/tokens)"]}),"\n",(0,d.jsxs)(n.li,{children:[(0,d.jsx)(n.code,{children:"whisper-cli"})," (",(0,d.jsx)(n.code,{children:"whisper-cpp"}),"; uses ",(0,d.jsx)(n.code,{children:"WHISPER_CPP_MODEL"})," or the bundled tiny model)"]}),"\n",(0,d.jsxs)(n.li,{children:[(0,d.jsx)(n.code,{children:"whisper"})," (Python CLI; downloads models automatically)"]}),"\n"]}),"\n"]}),"\n",(0,d.jsxs)(n.li,{children:[(0,d.jsx)(n.strong,{children:"Gemini CLI"})," (",(0,d.jsx)(n.code,{children:"gemini"}),") using ",(0,d.jsx)(n.code,{children:"read_many_files"})]}),"\n",(0,d.jsxs)(n.li,{children:[(0,d.jsx)(n.strong,{children:"Provider keys"}),"\n",(0,d.jsxs)(n.ul,{children:["\n",(0,d.jsx)(n.li,{children:"Audio: OpenAI \u2192 Groq \u2192 Deepgram \u2192 Google"}),"\n",(0,d.jsx)(n.li,{children:"Image: OpenAI \u2192 Anthropic \u2192 Google \u2192 MiniMax"}),"\n",(0,d.jsx)(n.li,{children:"Video: Google"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,d.jsx)(n.p,{children:"To disable auto-detection, set:"}),"\n",(0,d.jsx)(n.pre,{children:(0,d.jsx)(n.code,{className:"language-json5",children:"{\n  tools: {\n    media: {\n      audio: {\n        enabled: false\n      }\n    }\n  }\n}\n"})}),"\n",(0,d.jsxs)(n.p,{children:["Note: Binary detection is best-effort across macOS/Linux/Windows; ensure the CLI is on ",(0,d.jsx)(n.code,{children:"PATH"})," (we expand ",(0,d.jsx)(n.code,{children:"~"}),"), or set an explicit CLI model with a full command path."]}),"\n",(0,d.jsx)(n.h2,{id:"capabilities-optional",children:"Capabilities (optional)"}),"\n",(0,d.jsxs)(n.p,{children:["If you set ",(0,d.jsx)(n.code,{children:"capabilities"}),", the entry only runs for those media types. For shared\nlists, Moltbot can infer defaults:"]}),"\n",(0,d.jsxs)(n.ul,{children:["\n",(0,d.jsxs)(n.li,{children:[(0,d.jsx)(n.code,{children:"openai"}),", ",(0,d.jsx)(n.code,{children:"anthropic"}),", ",(0,d.jsx)(n.code,{children:"minimax"}),": ",(0,d.jsx)(n.strong,{children:"image"})]}),"\n",(0,d.jsxs)(n.li,{children:[(0,d.jsx)(n.code,{children:"google"})," (Gemini API): ",(0,d.jsx)(n.strong,{children:"image + audio + video"})]}),"\n",(0,d.jsxs)(n.li,{children:[(0,d.jsx)(n.code,{children:"groq"}),": ",(0,d.jsx)(n.strong,{children:"audio"})]}),"\n",(0,d.jsxs)(n.li,{children:[(0,d.jsx)(n.code,{children:"deepgram"}),": ",(0,d.jsx)(n.strong,{children:"audio"})]}),"\n"]}),"\n",(0,d.jsxs)(n.p,{children:["For CLI entries, ",(0,d.jsxs)(n.strong,{children:["set ",(0,d.jsx)(n.code,{children:"capabilities"})," explicitly"]})," to avoid surprising matches.\nIf you omit ",(0,d.jsx)(n.code,{children:"capabilities"}),", the entry is eligible for the list it appears in."]}),"\n",(0,d.jsx)(n.h2,{id:"provider-support-matrix-moltbot-integrations",children:"Provider support matrix (Moltbot integrations)"}),"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",(0,d.jsxs)(n.table,{children:[(0,d.jsx)(n.thead,{children:(0,d.jsxs)(n.tr,{children:[(0,d.jsx)(n.th,{children:"Capability"}),(0,d.jsx)(n.th,{children:"Provider integration"}),(0,d.jsx)(n.th,{children:"Notes"})]})}),(0,d.jsxs)(n.tbody,{children:[(0,d.jsxs)(n.tr,{children:[(0,d.jsx)(n.td,{children:"Image"}),(0,d.jsxs)(n.td,{children:["OpenAI / Anthropic / Google / others via ",(0,d.jsx)(n.code,{children:"pi-ai"})]}),(0,d.jsx)(n.td,{children:"Any image-capable model in the registry works."})]}),(0,d.jsxs)(n.tr,{children:[(0,d.jsx)(n.td,{children:"Audio"}),(0,d.jsx)(n.td,{children:"OpenAI, Groq, Deepgram, Google"}),(0,d.jsx)(n.td,{children:"Provider transcription (Whisper/Deepgram/Gemini)."})]}),(0,d.jsxs)(n.tr,{children:[(0,d.jsx)(n.td,{children:"Video"}),(0,d.jsx)(n.td,{children:"Google (Gemini API)"}),(0,d.jsx)(n.td,{children:"Provider video understanding."})]})]})]}),"\n",(0,d.jsx)(n.h2,{id:"recommended-providers",children:"Recommended providers"}),"\n",(0,d.jsx)(n.p,{children:(0,d.jsx)(n.strong,{children:"Image"})}),"\n",(0,d.jsxs)(n.ul,{children:["\n",(0,d.jsx)(n.li,{children:"Prefer your active model if it supports images."}),"\n",(0,d.jsxs)(n.li,{children:["Good defaults: ",(0,d.jsx)(n.code,{children:"openai/gpt-5.2"}),", ",(0,d.jsx)(n.code,{children:"anthropic/claude-opus-4-5"}),", ",(0,d.jsx)(n.code,{children:"google/gemini-3-pro-preview"}),"."]}),"\n"]}),"\n",(0,d.jsx)(n.p,{children:(0,d.jsx)(n.strong,{children:"Audio"})}),"\n",(0,d.jsxs)(n.ul,{children:["\n",(0,d.jsxs)(n.li,{children:[(0,d.jsx)(n.code,{children:"openai/gpt-4o-mini-transcribe"}),", ",(0,d.jsx)(n.code,{children:"groq/whisper-large-v3-turbo"}),", or ",(0,d.jsx)(n.code,{children:"deepgram/nova-3"}),"."]}),"\n",(0,d.jsxs)(n.li,{children:["CLI fallback: ",(0,d.jsx)(n.code,{children:"whisper-cli"})," (whisper-cpp) or ",(0,d.jsx)(n.code,{children:"whisper"}),"."]}),"\n",(0,d.jsxs)(n.li,{children:["Deepgram setup: ",(0,d.jsx)(n.a,{href:"/providers/deepgram",children:"Deepgram (audio transcription)"}),"."]}),"\n"]}),"\n",(0,d.jsx)(n.p,{children:(0,d.jsx)(n.strong,{children:"Video"})}),"\n",(0,d.jsxs)(n.ul,{children:["\n",(0,d.jsxs)(n.li,{children:[(0,d.jsx)(n.code,{children:"google/gemini-3-flash-preview"})," (fast), ",(0,d.jsx)(n.code,{children:"google/gemini-3-pro-preview"})," (richer)."]}),"\n",(0,d.jsxs)(n.li,{children:["CLI fallback: ",(0,d.jsx)(n.code,{children:"gemini"})," CLI (supports ",(0,d.jsx)(n.code,{children:"read_file"})," on video/audio)."]}),"\n"]}),"\n",(0,d.jsx)(n.h2,{id:"attachment-policy",children:"Attachment policy"}),"\n",(0,d.jsxs)(n.p,{children:["Per\u2011capability ",(0,d.jsx)(n.code,{children:"attachments"})," controls which attachments are processed:"]}),"\n",(0,d.jsxs)(n.ul,{children:["\n",(0,d.jsxs)(n.li,{children:[(0,d.jsx)(n.code,{children:"mode"}),": ",(0,d.jsx)(n.code,{children:"first"})," (default) or ",(0,d.jsx)(n.code,{children:"all"})]}),"\n",(0,d.jsxs)(n.li,{children:[(0,d.jsx)(n.code,{children:"maxAttachments"}),": cap the number processed (default ",(0,d.jsx)(n.strong,{children:"1"}),")"]}),"\n",(0,d.jsxs)(n.li,{children:[(0,d.jsx)(n.code,{children:"prefer"}),": ",(0,d.jsx)(n.code,{children:"first"}),", ",(0,d.jsx)(n.code,{children:"last"}),", ",(0,d.jsx)(n.code,{children:"path"}),", ",(0,d.jsx)(n.code,{children:"url"})]}),"\n"]}),"\n",(0,d.jsxs)(n.p,{children:["When ",(0,d.jsx)(n.code,{children:'mode: "all"'}),", outputs are labeled ",(0,d.jsx)(n.code,{children:"[Image 1/2]"}),", ",(0,d.jsx)(n.code,{children:"[Audio 2/2]"}),", etc."]}),"\n",(0,d.jsx)(n.h2,{id:"config-examples",children:"Config examples"}),"\n",(0,d.jsx)(n.h3,{id:"1-shared-models-list--overrides",children:"1) Shared models list + overrides"}),"\n",(0,d.jsx)(n.pre,{children:(0,d.jsx)(n.code,{className:"language-json5",children:'{\n  tools: {\n    media: {\n      models: [\n        { provider: "openai", model: "gpt-5.2", capabilities: ["image"] },\n        { provider: "google", model: "gemini-3-flash-preview", capabilities: ["image", "audio", "video"] },\n        {\n          type: "cli",\n          command: "gemini",\n          args: [\n            "-m",\n            "gemini-3-flash",\n            "--allowed-tools",\n            "read_file",\n            "Read the media at {{MediaPath}} and describe it in <= {{MaxChars}} characters."\n          ],\n          capabilities: ["image", "video"]\n        }\n      ],\n      audio: {\n        attachments: { mode: "all", maxAttachments: 2 }\n      },\n      video: {\n        maxChars: 500\n      }\n    }\n  }\n}\n'})}),"\n",(0,d.jsx)(n.h3,{id:"2-audio--video-only-image-off",children:"2) Audio + Video only (image off)"}),"\n",(0,d.jsx)(n.pre,{children:(0,d.jsx)(n.code,{className:"language-json5",children:'{\n  tools: {\n    media: {\n      audio: {\n        enabled: true,\n        models: [\n          { provider: "openai", model: "gpt-4o-mini-transcribe" },\n          {\n            type: "cli",\n            command: "whisper",\n            args: ["--model", "base", "{{MediaPath}}"]\n          }\n        ]\n      },\n      video: {\n        enabled: true,\n        maxChars: 500,\n        models: [\n          { provider: "google", model: "gemini-3-flash-preview" },\n          {\n            type: "cli",\n            command: "gemini",\n            args: [\n              "-m",\n              "gemini-3-flash",\n              "--allowed-tools",\n              "read_file",\n              "Read the media at {{MediaPath}} and describe it in <= {{MaxChars}} characters."\n            ]\n          }\n        ]\n      }\n    }\n  }\n}\n'})}),"\n",(0,d.jsx)(n.h3,{id:"3-optional-image-understanding",children:"3) Optional image understanding"}),"\n",(0,d.jsx)(n.pre,{children:(0,d.jsx)(n.code,{className:"language-json5",children:'{\n  tools: {\n    media: {\n      image: {\n        enabled: true,\n        maxBytes: 10485760,\n        maxChars: 500,\n        models: [\n          { provider: "openai", model: "gpt-5.2" },\n          { provider: "anthropic", model: "claude-opus-4-5" },\n          {\n            type: "cli",\n            command: "gemini",\n            args: [\n              "-m",\n              "gemini-3-flash",\n              "--allowed-tools",\n              "read_file",\n              "Read the media at {{MediaPath}} and describe it in <= {{MaxChars}} characters."\n            ]\n          }\n        ]\n      }\n    }\n  }\n}\n'})}),"\n",(0,d.jsx)(n.h3,{id:"4-multimodal-single-entry-explicit-capabilities",children:"4) Multi\u2011modal single entry (explicit capabilities)"}),"\n",(0,d.jsx)(n.pre,{children:(0,d.jsx)(n.code,{className:"language-json5",children:'{\n  tools: {\n    media: {\n      image: { models: [{ provider: "google", model: "gemini-3-pro-preview", capabilities: ["image", "video", "audio"] }] },\n      audio: { models: [{ provider: "google", model: "gemini-3-pro-preview", capabilities: ["image", "video", "audio"] }] },\n      video: { models: [{ provider: "google", model: "gemini-3-pro-preview", capabilities: ["image", "video", "audio"] }] }\n    }\n  }\n}\n'})}),"\n",(0,d.jsx)(n.h2,{id:"status-output",children:"Status output"}),"\n",(0,d.jsxs)(n.p,{children:["When media understanding runs, ",(0,d.jsx)(n.code,{children:"/status"})," includes a short summary line:"]}),"\n",(0,d.jsx)(n.pre,{children:(0,d.jsx)(n.code,{children:"\ud83d\udcce Media: image ok (openai/gpt-5.2) \xb7 audio skipped (maxBytes)\n"})}),"\n",(0,d.jsx)(n.p,{children:"This shows per\u2011capability outcomes and the chosen provider/model when applicable."}),"\n",(0,d.jsx)(n.h2,{id:"notes",children:"Notes"}),"\n",(0,d.jsxs)(n.ul,{children:["\n",(0,d.jsxs)(n.li,{children:["Understanding is ",(0,d.jsx)(n.strong,{children:"best\u2011effort"}),". Errors do not block replies."]}),"\n",(0,d.jsx)(n.li,{children:"Attachments are still passed to models even when understanding is disabled."}),"\n",(0,d.jsxs)(n.li,{children:["Use ",(0,d.jsx)(n.code,{children:"scope"})," to limit where understanding runs (e.g. only DMs)."]}),"\n"]}),"\n",(0,d.jsx)(n.h2,{id:"related-docs",children:"Related docs"}),"\n",(0,d.jsxs)(n.ul,{children:["\n",(0,d.jsx)(n.li,{children:(0,d.jsx)(n.a,{href:"/gateway/configuration",children:"Configuration"})}),"\n",(0,d.jsx)(n.li,{children:(0,d.jsx)(n.a,{href:"/nodes/images",children:"Image & Media Support"})}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,l.R)(),...e.components};return n?(0,d.jsx)(n,{...e,children:(0,d.jsx)(c,{...e})}):c(e)}},28453(e,n,i){i.d(n,{R:()=>r,x:()=>o});var s=i(96540);const d={},l=s.createContext(d);function r(e){const n=s.useContext(l);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(d):e.components||d:r(e.components),s.createElement(l.Provider,{value:n},e.children)}}}]);