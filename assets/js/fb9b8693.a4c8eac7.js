"use strict";(globalThis.webpackChunkdocs_workspace=globalThis.webpackChunkdocs_workspace||[]).push([[9644],{61270(e,n,i){i.r(n),i.d(n,{assets:()=>t,contentTitle:()=>l,default:()=>h,frontMatter:()=>r,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"nodes/audio","title":"Audio / Voice Notes \u2014 2026-01-17","description":"What works","source":"@site/content/moltbot/docs/nodes/audio.md","sourceDirName":"nodes","slug":"/nodes/audio","permalink":"/moltbot-doc/docs/nodes/audio","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"summary":"How inbound audio/voice notes are downloaded, transcribed, and injected into replies","read_when":["Changing audio transcription or media handling"]}}');var s=i(74848),d=i(28453);const r={summary:"How inbound audio/voice notes are downloaded, transcribed, and injected into replies",read_when:["Changing audio transcription or media handling"]},l="Audio / Voice Notes \u2014 2026-01-17",t={},c=[{value:"What works",id:"what-works",level:2},{value:"Auto-detection (default)",id:"auto-detection-default",level:2},{value:"Config examples",id:"config-examples",level:2},{value:"Provider + CLI fallback (OpenAI + Whisper CLI)",id:"provider--cli-fallback-openai--whisper-cli",level:3},{value:"Provider-only with scope gating",id:"provider-only-with-scope-gating",level:3},{value:"Provider-only (Deepgram)",id:"provider-only-deepgram",level:3},{value:"Notes &amp; limits",id:"notes--limits",level:2},{value:"Gotchas",id:"gotchas",level:2}];function a(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,d.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"audio--voice-notes--2026-01-17",children:"Audio / Voice Notes \u2014 2026-01-17"})}),"\n",(0,s.jsx)(n.h2,{id:"what-works",children:"What works"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Media understanding (audio)"}),": If audio understanding is enabled (or auto\u2011detected), Moltbot:\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Locates the first audio attachment (local path or URL) and downloads it if needed."}),"\n",(0,s.jsxs)(n.li,{children:["Enforces ",(0,s.jsx)(n.code,{children:"maxBytes"})," before sending to each model entry."]}),"\n",(0,s.jsx)(n.li,{children:"Runs the first eligible model entry in order (provider or CLI)."}),"\n",(0,s.jsx)(n.li,{children:"If it fails or skips (size/timeout), it tries the next entry."}),"\n",(0,s.jsxs)(n.li,{children:["On success, it replaces ",(0,s.jsx)(n.code,{children:"Body"})," with an ",(0,s.jsx)(n.code,{children:"[Audio]"})," block and sets ",(0,s.jsx)(n.code,{children:"{{Transcript}}"}),"."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Command parsing"}),": When transcription succeeds, ",(0,s.jsx)(n.code,{children:"CommandBody"}),"/",(0,s.jsx)(n.code,{children:"RawBody"})," are set to the transcript so slash commands still work."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Verbose logging"}),": In ",(0,s.jsx)(n.code,{children:"--verbose"}),", we log when transcription runs and when it replaces the body."]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"auto-detection-default",children:"Auto-detection (default)"}),"\n",(0,s.jsxs)(n.p,{children:["If you ",(0,s.jsx)(n.strong,{children:"don\u2019t configure models"})," and ",(0,s.jsx)(n.code,{children:"tools.media.audio.enabled"})," is ",(0,s.jsx)(n.strong,{children:"not"})," set to ",(0,s.jsx)(n.code,{children:"false"}),",\nMoltbot auto-detects in this order and stops at the first working option:"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Local CLIs"})," (if installed)\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"sherpa-onnx-offline"})," (requires ",(0,s.jsx)(n.code,{children:"SHERPA_ONNX_MODEL_DIR"})," with encoder/decoder/joiner/tokens)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"whisper-cli"})," (from ",(0,s.jsx)(n.code,{children:"whisper-cpp"}),"; uses ",(0,s.jsx)(n.code,{children:"WHISPER_CPP_MODEL"})," or the bundled tiny model)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"whisper"})," (Python CLI; downloads models automatically)"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Gemini CLI"})," (",(0,s.jsx)(n.code,{children:"gemini"}),") using ",(0,s.jsx)(n.code,{children:"read_many_files"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Provider keys"})," (OpenAI \u2192 Groq \u2192 Deepgram \u2192 Google)"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["To disable auto-detection, set ",(0,s.jsx)(n.code,{children:"tools.media.audio.enabled: false"}),".\nTo customize, set ",(0,s.jsx)(n.code,{children:"tools.media.audio.models"}),".\nNote: Binary detection is best-effort across macOS/Linux/Windows; ensure the CLI is on ",(0,s.jsx)(n.code,{children:"PATH"})," (we expand ",(0,s.jsx)(n.code,{children:"~"}),"), or set an explicit CLI model with a full command path."]}),"\n",(0,s.jsx)(n.h2,{id:"config-examples",children:"Config examples"}),"\n",(0,s.jsx)(n.h3,{id:"provider--cli-fallback-openai--whisper-cli",children:"Provider + CLI fallback (OpenAI + Whisper CLI)"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-json5",children:'{\n  tools: {\n    media: {\n      audio: {\n        enabled: true,\n        maxBytes: 20971520,\n        models: [\n          { provider: "openai", model: "gpt-4o-mini-transcribe" },\n          {\n            type: "cli",\n            command: "whisper",\n            args: ["--model", "base", "{{MediaPath}}"],\n            timeoutSeconds: 45\n          }\n        ]\n      }\n    }\n  }\n}\n'})}),"\n",(0,s.jsx)(n.h3,{id:"provider-only-with-scope-gating",children:"Provider-only with scope gating"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-json5",children:'{\n  tools: {\n    media: {\n      audio: {\n        enabled: true,\n        scope: {\n          default: "allow",\n          rules: [\n            { action: "deny", match: { chatType: "group" } }\n          ]\n        },\n        models: [\n          { provider: "openai", model: "gpt-4o-mini-transcribe" }\n        ]\n      }\n    }\n  }\n}\n'})}),"\n",(0,s.jsx)(n.h3,{id:"provider-only-deepgram",children:"Provider-only (Deepgram)"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-json5",children:'{\n  tools: {\n    media: {\n      audio: {\n        enabled: true,\n        models: [{ provider: "deepgram", model: "nova-3" }]\n      }\n    }\n  }\n}\n'})}),"\n",(0,s.jsx)(n.h2,{id:"notes--limits",children:"Notes & limits"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Provider auth follows the standard model auth order (auth profiles, env vars, ",(0,s.jsx)(n.code,{children:"models.providers.*.apiKey"}),")."]}),"\n",(0,s.jsxs)(n.li,{children:["Deepgram picks up ",(0,s.jsx)(n.code,{children:"DEEPGRAM_API_KEY"})," when ",(0,s.jsx)(n.code,{children:'provider: "deepgram"'})," is used."]}),"\n",(0,s.jsxs)(n.li,{children:["Deepgram setup details: ",(0,s.jsx)(n.a,{href:"/providers/deepgram",children:"Deepgram (audio transcription)"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:["Audio providers can override ",(0,s.jsx)(n.code,{children:"baseUrl"}),", ",(0,s.jsx)(n.code,{children:"headers"}),", and ",(0,s.jsx)(n.code,{children:"providerOptions"})," via ",(0,s.jsx)(n.code,{children:"tools.media.audio"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:["Default size cap is 20MB (",(0,s.jsx)(n.code,{children:"tools.media.audio.maxBytes"}),"). Oversize audio is skipped for that model and the next entry is tried."]}),"\n",(0,s.jsxs)(n.li,{children:["Default ",(0,s.jsx)(n.code,{children:"maxChars"})," for audio is ",(0,s.jsx)(n.strong,{children:"unset"})," (full transcript). Set ",(0,s.jsx)(n.code,{children:"tools.media.audio.maxChars"})," or per-entry ",(0,s.jsx)(n.code,{children:"maxChars"})," to trim output."]}),"\n",(0,s.jsxs)(n.li,{children:["OpenAI auto default is ",(0,s.jsx)(n.code,{children:"gpt-4o-mini-transcribe"}),"; set ",(0,s.jsx)(n.code,{children:'model: "gpt-4o-transcribe"'})," for higher accuracy."]}),"\n",(0,s.jsxs)(n.li,{children:["Use ",(0,s.jsx)(n.code,{children:"tools.media.audio.attachments"})," to process multiple voice notes (",(0,s.jsx)(n.code,{children:'mode: "all"'})," + ",(0,s.jsx)(n.code,{children:"maxAttachments"}),")."]}),"\n",(0,s.jsxs)(n.li,{children:["Transcript is available to templates as ",(0,s.jsx)(n.code,{children:"{{Transcript}}"}),"."]}),"\n",(0,s.jsx)(n.li,{children:"CLI stdout is capped (5MB); keep CLI output concise."}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"gotchas",children:"Gotchas"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Scope rules use first-match wins. ",(0,s.jsx)(n.code,{children:"chatType"})," is normalized to ",(0,s.jsx)(n.code,{children:"direct"}),", ",(0,s.jsx)(n.code,{children:"group"}),", or ",(0,s.jsx)(n.code,{children:"room"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:["Ensure your CLI exits 0 and prints plain text; JSON needs to be massaged via ",(0,s.jsx)(n.code,{children:"jq -r .text"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:["Keep timeouts reasonable (",(0,s.jsx)(n.code,{children:"timeoutSeconds"}),", default 60s) to avoid blocking the reply queue."]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,d.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(a,{...e})}):a(e)}},28453(e,n,i){i.d(n,{R:()=>r,x:()=>l});var o=i(96540);const s={},d=o.createContext(s);function r(e){const n=o.useContext(d);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),o.createElement(d.Provider,{value:n},e.children)}}}]);